# 梯度下降与优化
1. **梯度下降核心**：以当前位置**负梯度**为最快下降方向更新参数；收敛条件：损失函数变化极微/达最大迭代次数。
2. **梯度下降3类方法**：
   - 批量梯度下降（BGD）：用全量数据，稳定但样本多则更新慢；
   - 随机梯度下降（SGD）：用单个样本，快但不稳定、对噪声敏感；
   - 小批量梯度下降（MBGD）：用batchsize样本，平衡速度与稳定性。
3. **优化关键问题**：
   - 凸/非凸：非凸问题存在局部最优（如神经网络损失函数）；
   - 梯度消失：多层网络梯度连乘趋近0，下层参数难训练（如logistic/tanh导数过小）。
4. **梯度下降改进**：
   - 学习率自适应：Adagrad、Adadelta、RMSprop；
   - 动量类：Momentum（负梯度加权移动平均）、NAG；
   - 梯度截断（缓解梯度问题）。
5. **超参数**：层数、每层神经元数、激活函数、学习率、正则化系数、mini-batch大小。
6. **其他优化算法**：网格/随机搜索、贝叶斯优化、神经架构搜索。
7. **泛化能力（正则化）**：
   - 增加约束：数据增强、标签平滑、L1/L2正则；
   - 干扰过程：early stop、权重衰减、SGD、Dropout。