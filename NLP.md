# 自然语言处理复习知识点

## 一、汉语自然语言处理基础 ⭐⭐⭐

### 1.1 核心技术

**中文分词**：中文词汇自动切分技术（NLP的基础任务）

**为什么中文需要分词？**（简答可能考）

* 中文词与词之间没有空格分隔
* 英文单词天然以空格分隔
* 分词是中文NLP的第一步

---

### 1.2 两大核心难题 ⭐⭐⭐

#### （1）切分歧义（简答重点）

| 歧义类型      | 定义         | 举例                          |
| --------- | ---------- | --------------------------- |
| **交集型歧义** | 两个词有重叠字符   | "中国/科学院" vs "中国/科学/院士"      |
| **组合型歧义** | 字符可组合成不同词  | "这/是/个/问题" vs "这是/个/问题"     |
| **混合型歧义** | 交集+组合的复合情况 | "在/北京大学/读书" vs "在/北京/大学/读书" |

**记忆技巧**：

* 交集型：字符**交叉重叠**
* 组合型：字符**重新组合**
* 混合型：**两者混合**

#### （2）新词识别（未登录词识别）

**包含类型**：

* ✓ 数字识别（如：2024年、3.14）
* ✓ 命名实体识别（人名、地名、机构名等）
* ✓ 形式词识别
* ✓ 离合词识别

---

## 二、三大分词流派 ⭐⭐⭐

### 2.1 对比总结（简答必考）

| 分词方法                | 核心思想    | 优点            | 缺点                        | 应用状态 |
| ------------------- | ------- | ------------- | ------------------------- | ---- |
| **机械分词法**<br>（基于词典） | 字符串匹配   | 简单实用<br>速度快   | 对词典依赖强<br>效果无保障<br>无法识别新词 | 广泛应用 |
| **基于语法和规则**         | 语言学规则   | 符合语言学理论       | 缺乏统一标准<br>效果不佳            | 试验阶段 |
| **基于统计**            | 概率和机器学习 | 适应性强<br>可处理歧义 | 需要大量语料<br>训练成本高           | 主流方法 |

### 2.2 机械分词法（基于词典）⭐⭐

**原理**：在词典中查找匹配

**常见算法**：

* 正向最大匹配（FMM）
* 逆向最大匹配（BMM）
* 双向最大匹配

**优缺点分析**（简答重点）：

* ✅ 优点：实现简单、速度快、易维护
* ❌ 缺点：

  * 强依赖词典质量
  * 无法处理未登录词
  * 歧义消解能力弱

### 2.3 基于统计的分词法 ⭐⭐⭐

**两大核心方法**：

#### （1）最大概率法

选择概率最大的分词方案

#### （2）标注法（序列标注）

将分词转化为字符标注问题

**常用机器学习算法**（简答可能考）：

```
传统方法：
├─ HMM（隐马尔可夫模型）
├─ MEMM（最大熵隐马模型）
├─ CRFs（条件随机场）⭐最常用
├─ SVM（支持向量机）
└─ Maximum Entropy（最大熵）

深度学习方法：
└─ BiLSTM-CRF
```

**CRFs的优势**（简答可能出现）：

* 考虑上下文特征
* 全局最优解
* 避免标注偏置问题

---

## 三、词表征与词嵌入 ⭐⭐⭐

### 3.1 TF-IDF方法 ⭐⭐⭐

#### 核心思想（简答重点）

> 一个词的重要性：
>
> * **正比于**：在单篇文档中的出现次数（TF）
> * **反比于**：在整个语料库中的出现频率（IDF）

#### 关键公式（计算必考）⭐⭐⭐

**（1）TF（词频，Term Frequency）**

$$TF = \frac{N}{M}$$

* $N$：该词在**本文档**中的出现次数
* $M$：文档中**所有词**的出现次数总和

**（2）IDF（逆文档频率，Inverse Document Frequency）**

$$IDF = \log\frac{D}{D_w}$$

* $D$：语料库中的**文档总数**
* $D_w$：包含词$w$的**文档数**

**（3）TF-IDF权重**

$$TF\text{-}IDF = TF \times IDF$$

#### 计算示例（必须掌握）⭐⭐⭐

**例题**：

* 语料库共100篇文档（D=100）
* 词"机器学习"在某文档出现3次（N=3）
* 该文档共50个词（M=50）
* "机器学习"在20篇文档中出现（D_w=20）

**计算**：
$$TF = \frac{3}{50} = 0.06$$

$$IDF = \log\frac{100}{20} = \log5 \approx 0.699$$

$$TF\text{-}IDF = 0.06 \times 0.699 \approx 0.042$$

#### TF-IDF的优缺点（简答可能考）

**优点**：

* ✅ 简单高效
* ✅ 易于理解和实现
* ✅ 适用于关键词提取、文档相似度计算

**缺点**：

* ❌ 不考虑词序
* ❌ 不考虑语义
* ❌ 对常见词惩罚不足

---

### 3.2 词嵌入（Word Embeddings）⭐⭐⭐

#### 定义（简答重点）

**词嵌入**：将高维词汇空间嵌入到低维连续向量空间的技术

* 每个单词/词组 → 实数域向量
* 语义相近的词 → 向量距离相近

**为什么需要词嵌入？**（简答可能考）

1. **降维**：词汇空间维度过高（几万到几十万）
2. **语义表示**：传统one-hot编码无法表示语义
3. **泛化能力**：相似词共享向量空间

#### 分类（按是否考虑语序）⭐⭐

| 类别        | 代表方法                                          | 特点             |
| --------- | --------------------------------------------- | -------------- |
| **不考虑语序** | 词袋模型（BOW）<br>TF-IDF                           | 简单高效<br>丢失语序信息 |
| **考虑语序**  | Word2Vec（CBOW、Skip-Gram）<br>GloVe<br>FastText | 保留上下文<br>语义更丰富 |

---

### 3.3 词袋模型（BOW）vs 词嵌入 ⭐⭐

#### 对比分析（简答可能考）

| 对比项      | 词袋模型（BOW）      | 词嵌入（Word Embeddings） |
| -------- | -------------- | -------------------- |
| **表示方式** | 稀疏向量（维度=词汇表大小） | 密集向量（通常50-300维）      |
| **语序**   | 不考虑            | 可考虑（CBOW/Skip-Gram）  |
| **语义**   | 无语义信息          | 语义相近词距离相近            |
| **维度**   | 高维（几万维）        | 低维（几百维）              |
| **训练**   | 无需训练           | 需要神经网络训练             |

#### 举例说明

**句子**："我爱自然语言处理"

**BOW表示**（假设词汇表1000个词）：

```
[0, 0, ..., 1(我), 0, ..., 1(爱), 0, ..., 1(自然), 1(语言), 1(处理), ..., 0]
维度：1000维，只有5个非零值
```

**词嵌入表示**：

```
我:      [0.2, -0.5, 0.8, ...]  (100维)
爱:      [0.1, 0.3, -0.2, ...]  (100维)
自然:    [0.5, 0.1, 0.3, ...]   (100维)
语言:    [0.6, 0.2, 0.4, ...]   (100维，与"自然"相近)
处理:    [0.3, 0.4, -0.1, ...]  (100维)
```

---

### 3.4 Word2Vec核心算法 ⭐⭐

#### CBOW vs Skip-Gram（简答可能考）

| 算法            | 全称                      | 核心思想      | 输入   | 输出   |
| ------------- | ----------------------- | --------- | ---- | ---- |
| **CBOW**      | Continuous Bag of Words | 用上下文预测中心词 | 上下文词 | 中心词  |
| **Skip-Gram** | -                       | 用中心词预测上下文 | 中心词  | 上下文词 |

**图示理解**：

```
句子："我 爱 自然 语言 处理"
窗口大小=2

CBOW：
输入：[我, 爱, 语言, 处理]  →  输出：[自然]
      (上下文)                    (中心词)

Skip-Gram：
输入：[自然]  →  输出：[我, 爱, 语言, 处理]
    (中心词)        (上下文)
```

**应用选择**：

* CBOW：速度快，适合大语料
* Skip-Gram：小语料效果好，对罕见词效果更佳

---

## 四、知识点体系总结

### 4.1 技术演进路线

```
传统方法（基于规则）
    ↓
统计方法（概率模型）
    ↓
机器学习（SVM, CRFs）
    ↓
深度学习（Word2Vec, BERT）
```

### 4.2 词表示方法演进

```
One-Hot编码
    ↓ (维度灾难)
TF-IDF
    ↓ (无语义)
词嵌入（Word2Vec）
    ↓ (静态表示)
上下文词向量（ELMo, BERT）
```

---

## 五、考试题型预测

### 5.1 简答题方向 ⭐⭐⭐

#### 必背题目

1. **中文分词的两大难题是什么？请举例说明**

   * 切分歧义（交集型、组合型、混合型+例子）
   * 新词识别（数字、命名实体等）

2. **对比三大分词流派的优缺点**

   * 机械分词法、基于规则、基于统计
   * 从原理、优点、缺点、应用状态四个维度

3. **TF-IDF的核心思想是什么？**

   * 词频正比、文档频率反比
   * 作用：提取关键词

4. **什么是词嵌入？为什么需要词嵌入？**

   * 定义：高维→低维映射
   * 原因：降维、语义表示、泛化能力

5. **对比BOW和词嵌入的区别**

   * 维度、语序、语义、训练需求

6. **对比CBOW和Skip-Gram**

   * 输入输出、应用场景

---

### 5.2 计算题方向 ⭐⭐⭐

#### 必练题型

**题型1：TF-IDF计算**（必考⭐⭐⭐）

**标准题目模板**：

```
语料库包含500篇文档，词"深度学习"的分布如下：
- 在文档A中出现5次
- 文档A共包含200个词
- "深度学习"在整个语料库中出现于25篇文档

计算"深度学习"在文档A中的TF-IDF值
```

**解题步骤**：

1. 计算TF = 5/200 = 0.025
2. 计算IDF = log(500/25) = log(20) ≈ 1.301
3. TF-IDF = 0.025 × 1.301 ≈ 0.033

---

**题型2：分词歧义判断**

**例题**：
判断以下分词歧义类型：

* "研究生命的起源"

  * "研究/生命/的/起源"
  * "研究生/命/的/起源"
  * **答案：交集型（"生"字重叠）**

---

### 5.3 考前速记清单 ✅

**必记公式**：

* [ ] TF = N/M
* [ ] IDF = log(D/Dw)
* [ ] TF-IDF = TF × IDF

**必记概念**：

* [ ] 三种切分歧义类型+例子
* [ ] 三大分词流派优缺点
* [ ] TF-IDF核心思想
* [ ] 词嵌入定义和作用
* [ ] CBOW vs Skip-Gram

**必记对比**：

* [ ] 机械分词 vs 统计分词
* [ ] BOW vs 词嵌入
* [ ] 考虑语序 vs 不考虑语序

---

## 六、学习建议

### ⭐⭐⭐ 必须掌握（考试必考）

* TF-IDF计算（公式+例题）
* 分词两大难题
* 三大分词流派对比
* 词嵌入的定义和分类

### ⭐⭐ 重点理解（可能出简答）

* 各类歧义的举例
* BOW vs 词嵌入对比
* CBOW vs Skip-Gram
* 词嵌入的必要性

### ⭐ 了解即可

* 具体算法细节（SVM、MEMM等）
* Word2Vec的神经网络结构

---

**复习策略**：

1. 先背公式和定义
2. 练习TF-IDF计算题（至少3道）
3. 整理简答题模板
4. 记忆对比表格
